{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Shows how to use the Converse API to stream a response from Anthropic Claude 3 Sonnet (on demand).\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def stream_conversation(bedrock_client,\n",
    "                    model_id,\n",
    "                    messages,\n",
    "                    system_prompts,\n",
    "                    inference_config,\n",
    "                    additional_model_fields):\n",
    "    \"\"\"\n",
    "    Sends messages to a model and streams the response.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send.\n",
    "        system_prompts (JSON) : The system prompts to send.\n",
    "        inference_config (JSON) : The inference configuration to use.\n",
    "        additional_model_fields (JSON) : Additional model fields to use.\n",
    "\n",
    "    Returns:\n",
    "        Nothing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    stream = response.get('stream')\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(\n",
    "                        f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(\n",
    "                        f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entrypoint for streaming message API response example.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    system_prompt = \"\"\"You are an app that creates playlists for a radio station\n",
    "      that plays rock and pop music. Only return song names and the artist.\"\"\"\n",
    "\n",
    "    # Message to send to the model.\n",
    "    input_text = \"Create a list of 3 pop songs.\"\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    \n",
    "    # System prompts.\n",
    "    system_prompts = [{\"text\" : system_prompt}]\n",
    "\n",
    "    # inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 200\n",
    "    # Base inference parameters.\n",
    "    inference_config = {\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    # Additional model inference parameters.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    try:\n",
    "        bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "        stream_conversation(bedrock_client, model_id, messages,\n",
    "                        system_prompts, inference_config, additional_model_fields)\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "              format(message))\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished streaming messages with model {model_id}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
